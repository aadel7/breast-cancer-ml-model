{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Breast Cancer is a universal major health issue which could be improved with early diagnosis which could potentially lead to a more effective treatment plan. \n",
    "\n",
    "The primary goal of this project is to develop a machine learning model that can accurately classify a breast tumor as either benign or malignant. This classification will be based on different features extracted from digitized images of a breast mass.\n",
    "\n",
    "Reference:\n",
    "\n",
    "*[1] Wolberg,William, Mangasarian,Olvi, Street,Nick, and Street,W.. (1995). Breast Cancer Wisconsin (Diagnostic). UCI Machine Learning Repository. https://doi.org/10.24432/C5DW2B.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wisconsin Diagnostic Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wisconsin Diagnostic Breast Cancer dataset describes the measurements on cells in suspicious lumps in a women's breast. This dataset, obtained from the University of Wisconsin Hospitals, includes 30 feature variables and 1 target variable. \n",
    "\n",
    "The features, or input attributes, are numeric, and a binary target variable: M for Malignant and B for benign.\n",
    "\n",
    "Each cell-nucleus include 10 actual features:\n",
    "\n",
    "- Radius (mean of distances from center to points on the perimeter)\n",
    "- Texture (standard deviation of gray-scale values)\n",
    "- Perimeter\n",
    "- Area\n",
    "- Smoothness (local variation in radius lengths)\n",
    "- Compactness (perimeter^2 / area - 1.0)\n",
    "- Concavity (severity of concave portions of the contour)\n",
    "- Concave points (number of concave portions of the contour)\n",
    "- Symmetry \n",
    "- Fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "Now, the dataset will be loaded to check the data in detail. To accomplish this, the required libraries and packages will be installed.\n",
    "\n",
    "A virtual environment will be instantiated and activated, which will help in isolating dependencies and avoiding conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv myenv\n",
    "!.\\myenv\\Scripts\\activate\n",
    "!code ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Jupyter Notebook Kernel will need to be switched to the newly created environment using the Jupyter Notebook interface, and then all the required libraries will be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ipykernel matplotlib pandas numpy seaborn scikit-learn tensorflow scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "\n",
    "data_combined = pd.concat([X, y], axis=1)\n",
    "  \n",
    "# metadata \n",
    "#print(breast_cancer_wisconsin_diagnostic.metadata) \n",
    "  \n",
    "# variable information \n",
    "#print(breast_cancer_wisconsin_diagnostic.variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A data preview will be retrieved to allow for an initial examination of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few rows of the data\n",
    "print(\"\\nFirst few rows of Features and Target combined\")\n",
    "print(data_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dimensionality of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that access to the repository from UCIML has been successfully established and the data has been loaded. \n",
    "\n",
    "Next, the dimensionality of the dataset needs to be understood in terms of the number of rows and columns (features and target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the data\n",
    "\n",
    "print(\"Shape of X (features):\", X.shape)\n",
    "print(\"Shape of y (targets):\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The statistical properties of each attribute will be retrieved to gain insights into the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "descriptionDataCombined = data_combined.describe()\n",
    "\n",
    "print(descriptionDataCombined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a classification problem, the data will be analyzed to determine the number of observations for each class. If the classes are highly imbalanced, further actions will need to be taken during data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = data_combined.groupby('Diagnosis').size()\n",
    "print(class_counts)\n",
    "\n",
    "total = class_counts.sum()\n",
    "proportion_B = class_counts['B'] / total\n",
    "proportion_M = class_counts['M'] / total\n",
    "\n",
    "print(f\"\\nProportion of class B: {proportion_B:.2f}\")\n",
    "print(f\"Proportion of class M: {proportion_M:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calculating the correlation between the different variables, it will be possible to analyze if they are tightly coupled (highly correlated). This is important as it can affect the performance of the machine learning algorithms.\n",
    "\n",
    "The Pearson's Correlation Coefficient will be calculated, assuming a normal distribution in the attributes. Before doing so, the target variable needs to be converted to a numeric one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Diagnosis column from string to numerical values.\n",
    "data_combined['Diagnosis'] = data_combined['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Calculate the Pearson's Correlation Coefficient\n",
    "correlations = data_combined.corr(method='pearson')\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the large size of the table, variables that are highly correlated (greater than 0.75) will be retrieved by sorting and printing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack the correlation matrix\n",
    "correlation_pairs = correlations.unstack()\n",
    "\n",
    "# Sort the correlation pairs\n",
    "sorted_pairs = correlation_pairs.sort_values(kind=\"quicksort\", ascending=False)\n",
    "\n",
    "# Retrieve high correlations and exclude self-correlations, as they aren't relevant.\n",
    "high_correlations = sorted_pairs[(sorted_pairs != 1.0) & (sorted_pairs.abs() > 0.75)]\n",
    "\n",
    "print(high_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A correlation matrix will be plotted to achieve a better understanding of the data correlation [1].\n",
    "\n",
    "References:\n",
    "\n",
    "*[2] A. Ashfaq, \"Plot correlation matrix using pandas,\" Stack Overflow, Apr. 8, 2015. [Online]. Available: [Stack Overflow](https://stackoverflow.com/questions/29432629/plot-correlation-matrix-using-pandas)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Correlation Matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(correlations, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xticks(range(len(correlations.columns)), correlations.columns, rotation=90)\n",
    "plt.yticks(range(len(correlations.index)), correlations.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation matrix shows a highly correlated variables are the radius and perimeter. This makes sense due to its mathematical relationship, as the radius is directly proportional to the perimeter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skew of Univariate Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, an analysis of the attributes will be carried out to identify if there is any attribute with a skew, in order to consider it during the data preparation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = data_combined.skew()\n",
    "\n",
    "sorted_skew = skew.sort_values(ascending=False)\n",
    "\n",
    "negative_skew = sorted_skew[sorted_skew < 0]\n",
    "positive_skew = sorted_skew[sorted_skew > 0]\n",
    "higher_skew = sorted_skew[sorted_skew > 1]\n",
    "\n",
    "print(\"\\nNegative Skew\\n\")\n",
    "print(negative_skew)\n",
    "\n",
    "print(\"\\nPositive Skew\\n\")\n",
    "print(positive_skew)\n",
    "\n",
    "print(\"\\nHigher Skew\\n\")\n",
    "print(higher_skew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skew results show there is no negative skew as all attribute values are positive. Additionally, it can be observed that some features have a higher skew than the rest of the features, with values greater than 3. \n",
    "\n",
    "Histograms of each attribute will now be plotted to check their distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Histograms\n",
    "data_combined.hist(figsize=[20, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it is described in the dataset information, there are no missing values in features and the target so no major transformations are deemed necessary for the data, except for transforming the target column to a numeric format, as some models require a numeric target variable.\n",
    "\n",
    "In terms of data scaling, the data will be standardized. Models such as Logistic Regression, SVM, k-Nearest Neighbors, and Neural Networks utilize optimization algorithms that perform better when the data is standardized.\n",
    "\n",
    "Additionally, a feature selection technique will be implemented in the next steps, which also requires standardization to ensure that all features are treated equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Target variable transformation\n",
    "label_encoder = LabelEncoder()\n",
    "y_numeric = label_encoder.fit_transform(y.squeeze())\n",
    "\n",
    "# Print the transformed target variable to check if we have transformed it correctly\n",
    "print(\"Transformed Target Variables (First 10 elements):\", y_numeric[:10])\n",
    "\n",
    "# Data standardization\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the data set includes several features related to the same mass area, a feature importance will be applied to understand which specific features are most important for prediction in improve the model interpreatibiltiy by simplifying it.\n",
    "\n",
    "In this case feature importance will be applied using the Ensemble method Random Forest Classifier and Train Test Split resampling method. Features will be filtered based on their score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_numeric, test_size=0.3, random_state=42)\n",
    "\n",
    "importances = np.zeros(X_scaled.shape[1])\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print feature importances\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Plot feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importances from Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# Select top features (e.g., those with importance > 0.01)\n",
    "top_features = feature_importance_df[feature_importance_df['Importance'] > 0.01]['Feature']\n",
    "X_top = X[top_features]\n",
    "\n",
    "# Print the important features\n",
    "print(\"Selected Important Features:\")\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature importance plot shows that the most important features are related to the shape and size of the mass, such as area, concavity and permiter.\n",
    "\n",
    "Some measurements suggests that there is some redundancy in the the features dataset.\n",
    "\n",
    "Other features such as texture, compactness or smoothness provide less information for the classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preprocessing step, and in order to exclude irrelevant or redundant features, dimensionality reduction will be carried out using the Principal Component Analysis (PCA) technique and Train Test Split. This will allow the capture of the maximum variance in the dataset with a fewer number of components.\n",
    "\n",
    "The feature importance will be also considered in this step.\n",
    "\n",
    "Reference:\n",
    "\n",
    "*[3] \"Principal Component Analysis with Python,\" GeeksforGeeks, 2024. [Online]. Available: https://www.geeksforgeeks.org/principal-component-analysis-with-python/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Fetch dataset\n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "\n",
    "# Data (as pandas dataframes)\n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "\n",
    "# Convert target variable to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y_numeric = label_encoder.fit_transform(y.squeeze())\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_numeric, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X.columns\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Select top features (e.g., those with importance > 0.01)\n",
    "top_features = feature_importance_df[feature_importance_df['Importance'] > 0.01]['Feature']\n",
    "X_top = pd.DataFrame(X_scaled, columns=feature_names)[top_features]\n",
    "\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# Check the shape of the transformed data\n",
    "print(\"Shape of X_pca:\", X_train_pca.shape)\n",
    "\n",
    "# Print the explained variance ratio of each principal component\n",
    "print(\"Explained variance ratio of each principal component:\")\n",
    "for i, variance_ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"Principal Component {i+1}: {variance_ratio:.4f}\")\n",
    "\n",
    "# Print the total explained variance\n",
    "total_explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "print(f\"Total explained variance: {total_explained_variance:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that Principal Components 1 have significantly higher variance ratios compared to the rest, making them more important in capturing the variance of the dataset.\n",
    "\n",
    "Logistic Regression model will be trained and evaluated using only the first 5 principal components.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression with the first 5 principal components\n",
    "X_train_pca_5 = X_train_pca[:, :5]  # Use the first 5 PCs\n",
    "X_test_pca_5 = X_test_pca[:, :5]\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train_pca_5, y_train)\n",
    "y_pred_pca_5 = model.predict(X_test_pca_5)\n",
    "accuracy_pca_5 = accuracy_score(y_test, y_pred_pca_5)\n",
    "print(f\"Accuracy with the first 5 PCs: {accuracy_pca_5:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model's performance remains high with an accuracy of approximately 0.98 using only the first 5 principal components. This indicates that the additional principal components are not necessary for solving the classification task effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the transformed dataset with the selected components is already stored in the variable X_PCA, the resampling method K-Fold Cross-Validation will be performed to evaluate how well the different models perform with data that haven't been used to train them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to compare and select the best model based on its performance estimation across multiple folds.\n",
    "\n",
    "The performance of the different machine learning algorithms will be estimated using K-Fold Cross-Validation:\n",
    "- Logistic Regression\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Support Vector Machine\n",
    "- k-Nearest Neighbors\n",
    "- Neural Network\n",
    "\n",
    "All of them are *Supervised Learning Models* since the data is labeled. Supervised learning is appropriate for classification tasks, which is the focus of this project. The aim is to train the models to accurately classify new data by learning patterns from the labeled training data.\n",
    "\n",
    "The models will learn based on the input features and the output label (benign or malignant). This training process will eventually enable the models to make accurate predictions on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Standardize and PCA transformation in pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)),\n",
    "])\n",
    "\n",
    "X_pca = pipeline.fit_transform(X_scaled)\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(solver='liblinear')))\n",
    "models.append(('Decission Tree Classifier', DecisionTreeClassifier()))\n",
    "models.append(('Random Tree Classifier', RandomForestClassifier()))\n",
    "models.append(('Support Vector Machine', SVC()))\n",
    "models.append(('k-Nearest Neighbors', KNeighborsClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "seed = 7 \n",
    "\n",
    "for name, model in models:\n",
    "   kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "   cv_results = cross_val_score(model, X_pca, y_numeric, cv=kfold, scoring=scoring)\n",
    "   results.append(cv_results)\n",
    "   names.append(name)\n",
    "   msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "   print(msg)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(results, tick_labels=names)\n",
    "plt.title('Model Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although all models perform very well on the dataset, Logistic Regression and SVM models have demonstrated the best scores.\n",
    "\n",
    "Now, the performance of a Neural Network will be analyzed on the same dataset and with the same seed.\n",
    "The model will include several Dense layers which will use two different activation functions:\n",
    "- ReLU\n",
    "- Sigmoid\n",
    "\n",
    "*ReLU* will output the input directly if posistive, while *Sigmoid* squashes the input to a range between 0 and 1.\n",
    "\n",
    "The selected loss function is *Binary Cross-Entropy* as it is suitable for a Binary classification problem. It will quantify the error between the predicted and actual values.\n",
    "\n",
    "Reference:\n",
    "\n",
    "*[4] \"Keras Layers,\" Keras Documentation, 2024. [Online]. Available: https://keras.io/2.15/api/layers/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress all UserWarnings to improve readability\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "\n",
    "# Define a function to create the different Keras models\n",
    "def create_model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=5, activation='relu'))  # input_dim should be 10\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Added dropout layers to prevent overfitting\n",
    "def create_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=5, activation='relu'))  # input_dim should be 10\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Added l2 regularization and dropout layers to prevent overfitting\n",
    "def create_model_3():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=5, activation='relu',kernel_regularizer=l2(0.01))) # Added l2 regularization\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(10, activation='relu',kernel_regularizer=l2(0.01))) # Added l2 regularization\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "neural_network_models = []\n",
    "neural_network_models.append(('Neural Network Model 1', KerasClassifier(build_fn=create_model_1, epochs=150, batch_size=10, verbose=0)))\n",
    "neural_network_models.append(('Neural Network Model 2', KerasClassifier(build_fn=create_model_2, epochs=150, batch_size=10, verbose=0)))\n",
    "neural_network_models.append(('Neural Network Model 3', KerasClassifier(build_fn=create_model_3, epochs=150, batch_size=10, verbose=0)))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "seed = 7 \n",
    "\n",
    "for name, model in neural_network_models:\n",
    "   kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "   cv_results = cross_val_score(model, X_pca, y_numeric, cv=kfold, scoring=scoring)\n",
    "   results.append(cv_results)\n",
    "   names.append(name)\n",
    "   msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "   print(msg)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(results, tick_labels=names)\n",
    "plt.title('Model Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to prevent overfitting, a dropout layer has been added. This layer will randomly set some of the neurons to zero during the training phase, with a 50% chance for each neuron to be dropped, to achieve generalization in the model.\n",
    "\n",
    "Additionally, L2 regularization has been added to the dense layers to keep the weights small and reduce model complexity and overfitting.\n",
    "\n",
    "After implementing additional layers, dropout, and L2 regularization, the accuracy of the neural network model has improved. The accuracy of the neural network model is now comparable to the other models.\n",
    "\n",
    "The neural network model achieved an accuracy of approximately 0.98, which is similar to the accuracy achieved by the other models. This indicates that the neural network model is performing well on the breast cancer dataset.\n",
    "\n",
    "Some overfitting is observed in the last model, as it performs better than the previous two models. This conclusion is based on independent testing, which shows a slight reduction in performance. However, further investigation is not pursued as it can be attributed to random variations due to how the cross-validation splits the data.\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "*[5] R. Vij, \"Combating Overfitting with Dropout Regularization,\" Towards Data Science, 27-Apr-2020. [Online]. Available: https://towardsdatascience.com/combating-overfitting-with-dropout-regularization-f721e8712fbe.*\n",
    "\n",
    "*[6] J. Brownlee, \"How to Reduce Overfitting in Deep Learning with Weight Regularization,\" Machine Learning Mastery, 25-Aug-2020. [Online]. Available: https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelining and Data Leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage occurs when information from outside the training dataset is used to create the model. In order to avoid data leakages a pipeline is used to standardize the data and apply PCA before training the model. This pipelining has been used in previous steps as well.\n",
    "\n",
    "The pipeline is then evaluated using k-Fold Cross-Validation. For each fold, the data pipeline (including Standarization and PCA) is fitted on the training data. Once this is done, fitted transformations are applied to the test data within the same fold.\n",
    "\n",
    "The model training is then done on the transformed training data and the model evaluation on the transformed test data.\n",
    "\n",
    "The goal is to ensure that the test data is never seen during the training and it is only used during the model evaluation.\n",
    "\n",
    "Reference:\n",
    "\n",
    "*[7] \"Leakage (machine learning),\" Wikipedia, 13-May-2024. [Online]. Available: https://en.wikipedia.org/wiki/Leakage_(machine_learning).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Suppress all UserWarnings to improve readability\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "\n",
    "data_combined = pd.concat([X, y], axis=1)\n",
    "\n",
    "# Convert target variable to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y.squeeze())\n",
    "\n",
    "# Define a function to create the different Keras models\n",
    "def create_model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=5, activation='relu'))  # input_dim should be 10\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Added dropout layers to prevent overfitting\n",
    "def create_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=5, activation='relu'))  # input_dim should be 10\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Added l2 regularization and dropout layers to prevent overfitting\n",
    "def create_model_3():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=5, activation='relu',kernel_regularizer=l2(0.01))) # Added l2 regularization\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(10, activation='relu',kernel_regularizer=l2(0.01))) # Added l2 regularization\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Number of folds and random seed\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Define the k-fold cross-validator\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "\n",
    "# Prepare models within pipelines to avoid data leakage\n",
    "models = []\n",
    "models.append(('Logistic Regression', Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)), \n",
    "    ('classifier', LogisticRegression(solver='liblinear'))\n",
    "])))\n",
    "models.append(('Decision Tree Classifier', Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)), \n",
    "    ('classifier', DecisionTreeClassifier())\n",
    "])))\n",
    "models.append(('Random Forest Classifier', Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)), \n",
    "    ('classifier', RandomForestClassifier())\n",
    "])))\n",
    "models.append(('Support Vector Machine', Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)), \n",
    "    ('classifier', SVC())\n",
    "])))\n",
    "models.append(('k-Nearest Neighbors', Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)), \n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])))\n",
    "models.append(('Neural Network 1', Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)), \n",
    "    ('classifier', KerasClassifier(model=create_model_1, epochs=150, batch_size=10, verbose=0))\n",
    "])))\n",
    "models.append(('Neural Network 2', Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)), \n",
    "    ('classifier', KerasClassifier(model=create_model_2, epochs=150, batch_size=10, verbose=0))\n",
    "])))\n",
    "models.append(('Neural Network 3', Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)), \n",
    "    ('classifier', KerasClassifier(model=create_model_3, epochs=150, batch_size=10, verbose=0))\n",
    "])))\n",
    "\n",
    "# Evaluate each model in turn using cross-validation\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "\n",
    "for name, model in models:\n",
    "    cv_results = cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(20, 6))\n",
    "plt.boxplot(results, tick_labels=names)\n",
    "plt.title('Model Comparison')\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': names,\n",
    "    'Mean Accuracy': [result.mean() for result in results],\n",
    "    'Standard Deviation': [result.std() for result in results]\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"\\nModel Performance:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the first five PCA components, machine learning models tend to perform better. This improvement is not observed when all ten PCA components are used, making the additional components unnecessary for enhancing model performance. Although this finding is somewhat expected, it highlights the effectiveness of dimensionality reduction in optimizing models.\n",
    "\n",
    "Both Logistic Regression and Neural Networks are effective for breast cancer classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is implemented for Logistic Regression and Neural Network 1 models so that the best configuration can be found and used for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import warnings\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Suppress all UserWarnings to improve readability\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "\n",
    "# Convert target variable to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y.squeeze())\n",
    "\n",
    "# Define a function to create the Keras model\n",
    "def create_model_1(optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=5, activation='relu'))\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Number of folds and random seed\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Define the k-fold cross-validator\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "\n",
    "# Logistic Regression Pipeline\n",
    "pipeline_lr = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)), \n",
    "    ('classifier', LogisticRegression(solver='liblinear'))\n",
    "])\n",
    "\n",
    "# Grid search for Logistic Regression\n",
    "param_grid_lr = [\n",
    "    {'classifier__solver': ['liblinear'], 'classifier__penalty': ['l1', 'l2'], 'classifier__C': [0.01, 0.1, 1.0, 10, 100]},\n",
    "    {'classifier__solver': ['lbfgs', 'sag', 'newton-cg'], 'classifier__penalty': ['l2'], 'classifier__C': [0.01, 0.1, 1.0, 10, 100]},\n",
    "    {'classifier__solver': ['saga'], 'classifier__penalty': ['l1', 'l2', 'elasticnet'], 'classifier__C': [0.01, 0.1, 1.0, 10, 100], 'classifier__l1_ratio': [0.5]}\n",
    "]\n",
    "grid_search_lr = GridSearchCV(pipeline_lr, param_grid_lr, cv=kfold, scoring='accuracy')\n",
    "grid_search_lr.fit(X, y)\n",
    "best_lr = grid_search_lr.best_estimator_\n",
    "print(f\"Logistic Regression - Best hyperparameters: {grid_search_lr.best_params_}\")\n",
    "print(f\"Logistic Regression - Best cross-validation accuracy: {grid_search_lr.best_score_:.4f}\")\n",
    "\n",
    "# Neural Network 1 Pipeline\n",
    "pipeline_nn1 = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)), \n",
    "    ('classifier', KerasClassifier(model=create_model_1, epochs=150, batch_size=10, verbose=0))\n",
    "])\n",
    "\n",
    "# Grid search for Neural Network 1\n",
    "param_grid_nn1 = {\n",
    "    'classifier__optimizer': ['adam'],\n",
    "    'classifier__batch_size': [10, 20],\n",
    "    'classifier__epochs': [100, 150]\n",
    "}\n",
    "\n",
    "# Evaluate the grid search using cross-validation\n",
    "grid_search_nn1 = GridSearchCV(pipeline_nn1, param_grid_nn1, cv=kfold, scoring='accuracy')\n",
    "grid_search_nn1.fit(X, y)\n",
    "best_nn1 = grid_search_nn1.best_estimator_\n",
    "print(f\"Neural Network 1 - Best hyperparameters: {grid_search_nn1.best_params_}\")\n",
    "print(f\"Neural Network 1 - Best cross-validation accuracy:  {grid_search_nn1.best_score_:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the *Logistic Regression* model has the highest cross-validation accuracy, it will be used it to make predictions on the test set. \n",
    "\n",
    "On the other hand, Neural  Network model won't be used as it does require more computational resources and time to train and it provides the same or less  accuracy as the Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score and Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Confusion Matrix will be plotted and a classification report generated to get a final review of how the model actually performs with the data set in terms of precision and recall.\n",
    "\n",
    "Additionally, the F1 Score will be calculated to retrieve a single metric that includes Precision and Recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import confusion_matrix, f1_score, classification_report\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# Data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "\n",
    "# Convert target variable to numerical values\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y.squeeze())\n",
    "\n",
    "# Number of folds and random seed\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "# Define the k-fold cross-validator\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "\n",
    "# Prepare Logistic Regression model within a pipeline to avoid data leakage\n",
    "logistic_regression_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('pca', PCA(n_components=5)), \n",
    "    ('classifier', LogisticRegression(solver='liblinear', C=0.1, penalty='l2'))\n",
    "])\n",
    "\n",
    "# Evaluate the model using cross-validation\n",
    "scoring = 'accuracy'\n",
    "cv_results = cross_val_score(logistic_regression_pipeline, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "# Print cross-validation results\n",
    "print(\"Logistic Regression: %f (%f)\" % (cv_results.mean(), cv_results.std()))\n",
    "\n",
    "# Create a DataFrame to display the cross-validation results\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression'],\n",
    "    'Mean Accuracy': [cv_results.mean()],\n",
    "    'Standard Deviation': [cv_results.std()]\n",
    "})\n",
    "# Fit the model on the training data\n",
    "logistic_regression_pipeline.fit(X, y)\n",
    "y_pred = logistic_regression_pipeline.predict(X)\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y, y_pred)\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Print Classification Report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y, y_pred, target_names=label_encoder.classes_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the Confusion Matrix the model predicted correctly positive and negative cases, showing minimal error in false negatives and positives.\n",
    "\n",
    "- True Positives (TP) were 354.\n",
    "- True Negatives (TN) were 200\n",
    "- False Positives (FP) were 3 and False Negatives (FN) were 12.\n",
    "\n",
    "The high precision of 99% shows that the model has very few false positives, making it highly reliable in predicting positive cases. The high recall value underscores the model's ability to detect nearly all positive instances, with minimal false negatives. This balanced performance is reflected in the F1 Score, which combines both precision and recall into a single metric. Overall, these metrics confirm that the Logistic Regression model is performing very well on this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
