{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Coursework assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wisconsin Diagnostic Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wisconsin Diagnostic Breast Cancer dataset describes the measurements on cells in suspicious lumps in a women's breast. This dataset, obtained from the University of Wisconsin Hospitals, includes 30 feature variables and 1 target variable. \n",
    "\n",
    "The features, or input attributes, are numeric, and a binary target variable: M for Malignant and B for benign.\n",
    "\n",
    "Each cell-nucleus include 10 actual features:\n",
    "\n",
    "- Radius (mean of distances from center to points on the perimeter)\n",
    "- Texture (standard deviation of gray-scale values)\n",
    "- Perimeter\n",
    "- Area\n",
    "- Smoothness (local variation in radius lengths)\n",
    "- Compactness (perimeter^2 / area - 1.0)\n",
    "- Concavity (severity of concave portions of the contour)\n",
    "- Concave points (number of concave portions of the contour)\n",
    "- Symmetry \n",
    "- Fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "Now, we will load the dataset so we can check the data in detail. To do so, we'll install the required libraries and packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will to activate a virtual environment which will be helpful in isolating dependencies and avoid conflicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv myenv\n",
    "!.\\myenv\\Scripts\\activate\n",
    "!code ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to switch the Jupyter Notebook Kernel to the newly created environment using the Jupyter Notebook interface and then install all the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install ipykernel matplotlib pandas numpy seaborn scikit-learn tensorflow scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "\n",
    "data_combined = pd.concat([X, y], axis=1)\n",
    "  \n",
    "# metadata \n",
    "#print(breast_cancer_wisconsin_diagnostic.metadata) \n",
    "  \n",
    "# variable information \n",
    "#print(breast_cancer_wisconsin_diagnostic.variables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll retrieve a data preview of the data so we can have a first look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few rows of the data\n",
    "print(\"\\nFirst few rows of Features and Target combined\")\n",
    "print(data_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dimensionality of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we're able to access the repository from UCIML and the data is loaded. Now we need to understand the dimensionality of the dataset in terms of number of rows, columns (features and target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the data\n",
    "\n",
    "print(\"Shape of X (features):\", X.shape)\n",
    "print(\"Shape of y (targets):\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the statistical properties of each attribute in order to get some insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "descriptionDataCombined = data_combined.describe()\n",
    "\n",
    "print(descriptionDataCombined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a classification problem, we need to analyse the data in order to check how many observations do we have for each class. If they are highly imbalanced, then we'll need to take further actions in the data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = data_combined.groupby('Diagnosis').size()\n",
    "print(class_counts)\n",
    "\n",
    "total = class_counts.sum()\n",
    "proportion_B = class_counts['B'] / total\n",
    "proportion_M = class_counts['M'] / total\n",
    "\n",
    "print(f\"\\nProportion of class B: {proportion_B:.2f}\")\n",
    "print(f\"Proportion of class M: {proportion_M:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calculating the correlation between the different variables we'll be able to analyse if they are tightly coupled (highly correlated). This is important as it can affect the performance of the machine learning algorithms.\n",
    "\n",
    "Let's calculate the Pearson's Correlation Coefficient, assuming that we have a normal distribution in the attributes. Before that, we need to convert the target variable to a numeric one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Diagnosis column from string to numerical values.\n",
    "data_combined['Diagnosis'] = data_combined['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Calculate the Pearson's Correlation Coefficient\n",
    "correlations = data_combined.corr(method='pearson')\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the table size is big, let's retrieve those variables that are highly correlated (> 0.75) by sorting and printing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack the correlation matrix\n",
    "correlation_pairs = correlations.unstack()\n",
    "\n",
    "# Sort the correlation pairs\n",
    "sorted_pairs = correlation_pairs.sort_values(kind=\"quicksort\", ascending=False)\n",
    "\n",
    "# Retrieve high correlations and exclude self-correlations, as they aren't relevant.\n",
    "high_correlations = sorted_pairs[(sorted_pairs != 1.0) & (sorted_pairs.abs() > 0.75)]\n",
    "\n",
    "print(high_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a correlation matrix to achieve a better understanding of the data correlation\n",
    "\n",
    "Datasource: https://stackoverflow.com/questions/29432629/plot-correlation-matrix-using-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Correlation Matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(correlations, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xticks(range(len(correlations.columns)), correlations.columns, rotation=90)\n",
    "plt.yticks(range(len(correlations.index)), correlations.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the highly correlated variables are the radius and perimeter. This makes sense due to its mathematical relationship, as the radius is directly proportional to the perimeter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skew of Univariate Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check if there is any attribute with a skew, in order to consider it during our data preparation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = data_combined.skew()\n",
    "\n",
    "sorted_skew = skew.sort_values(ascending=False)\n",
    "\n",
    "negative_skew = sorted_skew[sorted_skew < 0]\n",
    "positive_skew = sorted_skew[sorted_skew > 0]\n",
    "higher_skew = sorted_skew[sorted_skew > 1]\n",
    "\n",
    "print(\"\\nNegative Skew\\n\")\n",
    "print(negative_skew)\n",
    "\n",
    "print(\"\\nPositive Skew\\n\")\n",
    "print(positive_skew)\n",
    "\n",
    "print(\"\\nHigher Skew\\n\")\n",
    "print(higher_skew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skew result shows there is no negative skew as all attribute values are positive. \n",
    "Additionally, we can see that some features have a higher skew than the rest of the features as they are greater than 3.\n",
    "Now, we'll plot the histograms of each attribute in order to check the distribution of each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Histograms\n",
    "data_combined.hist(figsize=[20, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see the need of any major rescaling or transformation in the data. What is needed, is to transform the target column, as it isn't numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_numeric = label_encoder.fit_transform(y.squeeze())\n",
    "\n",
    "# Print the transformed target variable to check if we have transformed it correctly\n",
    "print(\"Transformed Target Variables (First 10 elements):\", y_numeric[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preprocessing step, and in order to exclude irrelevant or redundant features, we will carry out a dimensionality reduction using the PCA technique. This will allow us to capture the maximum variance in our dataset with a fewer number of components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=10)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Check the shape of the transformed data\n",
    "print(\"Shape of X_pca:\", X_pca.shape)\n",
    "\n",
    "# Print the explained variance ratio of each principal component\n",
    "print(\"Explained variance ratio of each principal component:\")\n",
    "for i, variance_ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    print(f\"Principal Component {i+1}: {variance_ratio:.4f}\")\n",
    "\n",
    "# Print the total explained variance\n",
    "total_explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "print(f\"Total explained variance: {total_explained_variance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Principal Components 1 and 2 have a significantly higher variance ratios than the rest so which make them more important in capturing the variance of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have the transformed dataset with the two selected components stored in the variable X_PCA, we will perform the resampling method K-Fold Cross-Validation to know how well the different models perform with data that haven´t been used to train them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to compare and select the best model based on its performance estimation across multiple folds.\n",
    "\n",
    "We'll estimate the performance of 3 different machine learning algorithms using the K-Fold Cross-Validation:\n",
    "- Logistic Regression\n",
    "- Decision Tree Classifier\n",
    "- Random Forest Classifier\n",
    "- Support Vector Machine\n",
    "- k-Nearest Neighbors\n",
    "- Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "\n",
    "# prepare models\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(solver='liblinear')))\n",
    "models.append(('Decission Tree Classifier', DecisionTreeClassifier()))\n",
    "models.append(('Random Tree Classifier', RandomForestClassifier()))\n",
    "models.append(('Support Vector Machine', SVC()))\n",
    "models.append(('k-Nearest Neighbors', KNeighborsClassifier()))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "seed = 7 \n",
    "\n",
    "for name, model in models:\n",
    "   kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "   cv_results = cross_val_score(model, X_pca, y_numeric, cv=kfold, scoring=scoring)\n",
    "   results.append(cv_results)\n",
    "   names.append(name)\n",
    "   msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "   print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress all UserWarnings to improve readability\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "# Environment setting for tf.keras\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "\n",
    "# Define a function to create the different Keras models\n",
    "def create_model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=10, activation='relu'))  # input_dim should be 10\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Added dropout layers to prevent overfitting\n",
    "def create_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=10, activation='relu'))  # input_dim should be 10\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(10, activation='relu'))\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Added l2 regularization and dropout layers to prevent overfitting\n",
    "def create_model_3():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(20, input_dim=10, activation='relu',kernel_regularizer=l2(0.01))) # Added l2 regularization\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(10, activation='relu',kernel_regularizer=l2(0.01))) # Added l2 regularization\n",
    "    model.add(Dropout(0.5))  # Dropout layer to prevent overfitting\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "neural_network_models = []\n",
    "neural_network_models.append(('Neural Network Model 1', KerasClassifier(build_fn=create_model_1, epochs=150, batch_size=10, verbose=0)))\n",
    "neural_network_models.append(('Neural Network Model 2', KerasClassifier(build_fn=create_model_2, epochs=150, batch_size=10, verbose=0)))\n",
    "neural_network_models.append(('Neural Network Model 3', KerasClassifier(build_fn=create_model_3, epochs=150, batch_size=10, verbose=0)))\n",
    "\n",
    "# evaluate each model in turn\n",
    "results = []\n",
    "names = []\n",
    "scoring = 'accuracy'\n",
    "seed = 7 \n",
    "\n",
    "for name, model in neural_network_models:\n",
    "   kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "   cv_results = cross_val_score(model, X_pca, y_numeric, cv=kfold, scoring=scoring)\n",
    "   results.append(cv_results)\n",
    "   names.append(name)\n",
    "   msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "   print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding additional layers and dropout layers, the accuracy of the neural network model has improved. The accuracy of the neural network model is now comparable to the other models, such as Logistic Regression, Decision Tree Classifier, Random Forest Classifier, Support Vector Machine, and k-Nearest Neighbors. The neural network model achieved an accuracy of approximately 0.98, which is similar to the accuracy achieved by the other models. This indicates that the neural network model is performing well on the breast cancer dataset.\n",
    "\n",
    "We observe some overfitting in the last model, as it performs better than the previous two models. This conclusion comes from independent testing, which shows a slight reduction in performance. However we won't investigate further but it can be attributed to random variations due to how the cross-validation splits the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
