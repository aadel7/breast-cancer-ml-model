{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AML Coursework assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wisconsin Diagnostic Breast Cancer Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Wisconsin Diagnostic Breast Cancer dataset describes the measurements on cells in suspicious lumps in a women's breast. This dataset, obtained from the University of Wisconsin Hospitals, includes 30 feature variables and 1 target variable. \n",
    "\n",
    "The features, or input attributes, are numeric, and a binary target variable: M for Malignant and B for benign.\n",
    "\n",
    "Each cell-nucleus include 10 actual features:\n",
    "\n",
    "- Radius (mean of distances from center to points on the perimeter)\n",
    "- Texture (standard deviation of gray-scale values)\n",
    "- Perimeter\n",
    "- Area\n",
    "- Smoothness (local variation in radius lengths)\n",
    "- Compactness (perimeter^2 / area - 1.0)\n",
    "- Concavity (severity of concave portions of the contour)\n",
    "- Concave points (number of concave portions of the contour)\n",
    "- Symmetry \n",
    "- Fractal dimension (\"coastline approximation\" - 1)\n",
    "\n",
    "Now, we will load the dataset so we can check the data in detail. To do so, we'll install the required libraries and packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install matplotlib pandas numpy seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -U ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "breast_cancer_wisconsin_diagnostic = fetch_ucirepo(id=17) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X = breast_cancer_wisconsin_diagnostic.data.features \n",
    "y = breast_cancer_wisconsin_diagnostic.data.targets \n",
    "\n",
    "data_combined = pd.concat([X, y], axis=1)\n",
    "  \n",
    "# metadata \n",
    "#print(breast_cancer_wisconsin_diagnostic.metadata) \n",
    "  \n",
    "# variable information \n",
    "#print(breast_cancer_wisconsin_diagnostic.variables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll retrieve a data preview of the data so we can have a first look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few rows of the data\n",
    "print(\"\\nFirst few rows of Features and Target combined\")\n",
    "print(data_combined.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dimensionality of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we're able to access the repository from UCIML and the data is loaded. Now we need to understand the dimensionality of the dataset in terms of number of rows, columns (features and target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the data\n",
    "\n",
    "print(\"Shape of X (features):\", X.shape)\n",
    "print(\"Shape of y (targets):\", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's retrieve the statistical properties of each attribute in order to get some insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "descriptionDataCombined = data_combined.describe()\n",
    "\n",
    "print(descriptionDataCombined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this is a classification problem, we need to analyse the data in order to check how many observations do we have for each class. If they are highly imbalanced, then we'll need to take further actions in the data pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = data_combined.groupby('Diagnosis').size()\n",
    "print(class_counts)\n",
    "\n",
    "total = class_counts.sum()\n",
    "proportion_B = class_counts['B'] / total\n",
    "proportion_M = class_counts['M'] / total\n",
    "\n",
    "print(f\"\\nProportion of class B: {proportion_B:.2f}\")\n",
    "print(f\"Proportion of class M: {proportion_M:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features Correlation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calculating the correlation between the different variables we'll be able to analyse if they are tightly coupled (highly correlated). This is important as it can affect the performance of the machine learning algorithms.\n",
    "\n",
    "Let's calculate the Pearson's Correlation Coefficient, assuming that we have a normal distribution in the attributes. Before that, we need to convert the target variable to a numeric one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the Diagnosis column from string to numerical values.\n",
    "data_combined['Diagnosis'] = data_combined['Diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# Calculate the Pearson's Correlation Coefficient\n",
    "correlations = data_combined.corr(method='pearson')\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the table size is big, let's retrieve those variables that are highly correlated (> 0.75) by sorting and printing them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unstack the correlation matrix\n",
    "correlation_pairs = correlations.unstack()\n",
    "\n",
    "# Sort the correlation pairs\n",
    "sorted_pairs = correlation_pairs.sort_values(kind=\"quicksort\", ascending=False)\n",
    "\n",
    "# Retrieve high correlations and exclude self-correlations, as they aren't relevant.\n",
    "high_correlations = sorted_pairs[(sorted_pairs != 1.0) & (sorted_pairs.abs() > 0.75)]\n",
    "\n",
    "print(high_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot a correlation matrix to achieve a better understanding of the data correlation\n",
    "\n",
    "Datasource: https://stackoverflow.com/questions/29432629/plot-correlation-matrix-using-pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the Correlation Matrix\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(correlations, cmap='coolwarm', interpolation='nearest')\n",
    "plt.colorbar(label='Correlation')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.xticks(range(len(correlations.columns)), correlations.columns, rotation=90)\n",
    "plt.yticks(range(len(correlations.index)), correlations.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the highly correlated variables are the radius and perimeter. This makes sense due to its mathematical relationship, as the radius is directly proportional to the perimeter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skew of Univariate Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll check if there is any attribute with a skew, in order to consider it during our data preparation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skew = data_combined.skew()\n",
    "\n",
    "sorted_skew = skew.sort_values(ascending=False)\n",
    "\n",
    "negative_skew = sorted_skew[sorted_skew < 0]\n",
    "positive_skew = sorted_skew[sorted_skew > 0]\n",
    "higher_skew = sorted_skew[sorted_skew > 1]\n",
    "\n",
    "print(\"\\nNegative Skew\\n\")\n",
    "print(negative_skew)\n",
    "\n",
    "print(\"\\nPositive Skew\\n\")\n",
    "print(positive_skew)\n",
    "\n",
    "print(\"\\nHigher Skew\\n\")\n",
    "print(higher_skew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The skew result shows there is no negative skew as all attribute values are positive. \n",
    "Additionally, we can see that some features have a higher skew than the rest of the features as they are greater than 3.\n",
    "Now, we'll plot the histograms of each attribute in order to check the distribution of each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate Histograms\n",
    "data_combined.hist(figsize=[20, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't see the need of any major rescaling or transformation in the data. What is needed, is to transform the target column, as it isn't numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Classes: ['B' 'M']\n",
      "Numeric Codes: [0 1]\n",
      "Transformed Target Variable (First 10 elements): [1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_numeric = label_encoder.fit_transform(y.squeeze())\n",
    "\n",
    "# Print the transformed target variable to check if we have transformed it correctly\n",
    "print(\"Transformed Target Variables (First 10 elements):\", y_numeric[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a preprocessing step, and in order to exclude irrelevant or redundant features, we will carry out a dimensionality reduction using the PCA technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=10)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# Get the explained variance ratios\n",
    "explained_variance_ratios = fit.explained_variance_ratio_\n",
    "\n",
    "# Transform the explained variance ratios into percentages\n",
    "explained_variance_percentages = explained_variance_ratios * 100\n",
    "\n",
    "# Print the explained variance percentages\n",
    "for i, explained_variance_percentage in enumerate(explained_variance_percentages):\n",
    "    print(f\"Explained Variance for PC{i+1}: {explained_variance_percentage:.10f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the Principal Component 1 has a significantly higher variance than the rest so it makes sense to only keep it. Because of that, we will reduce the number of components to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# Get the explained variance ratios\n",
    "explained_variance_ratios = fit.explained_variance_ratio_\n",
    "\n",
    "# Transform the explained variance ratios into percentages\n",
    "explained_variance_percentages = explained_variance_ratios * 100\n",
    "\n",
    "# Print the explained variance percentages\n",
    "for i, explained_variance_percentage in enumerate(explained_variance_percentages):\n",
    "    print(f\"Explained Variance for PC{i+1}: {explained_variance_percentage:.10f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have two PCs, we will fit them to the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have the transformed dataset with the two selected components stored in the variable X_PCA, we will perform the resampling method K-Fold Cross-Validation to know how well the different models perform with data that havenÂ´t been used to train them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "num_folds = 10\n",
    "seed = 7\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "\n",
    "model = LogisticRegression(solver = 'liblinear')\n",
    "\n",
    "results = cross_val_score(model, X, y, cv=kfold)\n",
    "\n",
    "print(\"Accuracy: %.3f%% (%.3f%%)\" % (results.mean() * 100.0, results.std() * 100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
